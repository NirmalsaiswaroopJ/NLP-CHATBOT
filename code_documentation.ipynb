{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Medical Chatbot Application\n",
    "\n",
    "## Introduction\n",
    "\n",
    "This project is a Flask-based medical chatbot application that uses Natural Language Processing (NLP) models built with Keras. It predicts user intents based on trained models and provides appropriate responses from a predefined dataset stored in a JSON file.\n",
    "The chatbot interface allows users to interact via a web application, sending messages and receiving context-specific replies. It supports essential features such as login, chatbot responses, and appointment management."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the entire code to look at - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from flask import Flask, render_template, request, redirect, url_for\n",
    "\n",
    "# Load the trained model and supporting files\n",
    "model = load_model('keras_intent_model.h5')\n",
    "intents = json.loads(open('data.json').read())\n",
    "words = pickle.load(open('texts.pkl','rb'))\n",
    "classes = pickle.load(open('labels.pkl','rb'))\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # Tokenize the sentence and lemmatize each word\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n",
    "\n",
    "def bow(sentence, words, show_details=True):\n",
    "    # Convert sentence to bag of words\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(f\"Found in bag: {w}\")\n",
    "    return np.array(bag)\n",
    "\n",
    "def predict_class(sentence, model):\n",
    "    # Predict the class of the sentence\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n",
    "\n",
    "def get_response(ints, intents_json):\n",
    "    # Get the response for the predicted intent\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n",
    "\n",
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    if len(ints) == 0:  # Check if no intents are predicted\n",
    "        return \"Sorry, I didn't quite understand that. Could you try again?\"\n",
    "    res = get_response(ints, intents)  # Ensure correct function name is used\n",
    "    return res\n",
    "\n",
    "# Flask application setup\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n",
    "\n",
    "# Home Route (Chatbot interface)\n",
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n",
    "\n",
    "# Chatbot Response Route\n",
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    userText = request.args.get('msg')\n",
    "    return chatbot_response(userText)\n",
    "\n",
    "# Login Route\n",
    "@app.route(\"/login\", methods=[\"GET\", \"POST\"])\n",
    "def login():\n",
    "    return render_template(\"login.html\")\n",
    "\n",
    "# Appointments Route (After login)\n",
    "@app.route(\"/appointments\")\n",
    "def appointments():\n",
    "    # This is where you would show or create appointments\n",
    "    # Placeholder for now\n",
    "    return render_template(\"appointments.html\")\n",
    "\n",
    "# Logout Route (Optional)\n",
    "@app.route(\"/logout\")\n",
    "def logout():\n",
    "    return redirect(url_for('home'))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's break down the code to understand it in an easy way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Setup & Requirements\n",
    "\n",
    "To run this project, you need the following dependencies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required libraries\n",
    "!pip install flask keras nltk\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Importing Required Libraries\n",
    "The first step is importing essential libraries for NLP, model handling, and Flask server setup.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('popular')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import pickle\n",
    "import numpy as np\n",
    "import random\n",
    "import json\n",
    "from keras.models import load_model\n",
    "from flask import Flask, render_template, request, redirect, url_for"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "\n",
    "1. nltk: Used for NLP tasks like tokenization and lemmatization.\n",
    "2. pickle: Loads preprocessed data such as words and class labels.\n",
    "3. numpy: Handles numerical computations.\n",
    "4. random: Generates random responses from predefined responses.\n",
    "5. json: Parses the intents file containing chatbot data.\n",
    "6. Flask: Manages the web application and routing.\n",
    "7. keras.models.load_model: Loads the pre-trained intent classification model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Loading and Preprocessing\n",
    "\n",
    "The following code loads the trained model, intents file, and supporting files required for chatbot prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model and supporting files\n",
    "model = load_model('keras_intent_model.h5')\n",
    "intents = json.loads(open('data.json').read())\n",
    "words = pickle.load(open('texts.pkl','rb'))\n",
    "classes = pickle.load(open('labels.pkl','rb'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- model = load_model('keras_intent_model.h5'): Loads the trained Keras model used for intent classification.\n",
    "- intents = json.loads(open('data.json').read()): Reads the intents file containing response data in JSON format.\n",
    "- words = pickle.load(open('texts.pkl','rb')): Loads tokenized words used during model training.\n",
    "- classes = pickle.load(open('labels.pkl','rb')): Loads class labels corresponding to intents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Preprocessing Functions\n",
    "The following functions handle text preprocessing tasks, including tokenization, lemmatization, and creating a bag of words representation.\n",
    "\n",
    "### a. Cleaning Up Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Initialize the lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def clean_up_sentence(sentence):\n",
    "    # Tokenize the sentence and lemmatize each word\n",
    "    sentence_words = nltk.word_tokenize(sentence)\n",
    "    sentence_words = [lemmatizer.lemmatize(word.lower()) for word in sentence_words]\n",
    "    return sentence_words\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- WordNetLemmatizer(): Reduces words to their base forms (e.g., \"running\" → \"run\").\n",
    "- nltk.word_tokenize(sentence): Splits the sentence into individual words (tokens).\n",
    "- lemmatizer.lemmatize(word.lower()): Converts each word to lowercase and lemmatizes it for consistent processing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Creating a Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentence, words, show_details=True):\n",
    "    # Convert sentence to bag of words\n",
    "    sentence_words = clean_up_sentence(sentence)\n",
    "    bag = [0] * len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i, w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print(f\"Found in bag: {w}\")\n",
    "    return np.array(bag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "1. sentence_words = clean_up_sentence(sentence): Preprocesses the input sentence.\n",
    "2. bag = [0] * len(words): Initializes a zero-filled list of the same length as the vocabulary.\n",
    "3. if w == s: bag[i] = 1: Sets the corresponding index to 1 if the word exists in the sentence.\n",
    "4. np.array(bag): Converts the list into a NumPy array for model compatibility.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction and Response Functions\n",
    "These functions handle predicting the intent of the user's input and retrieving the appropriate response.\n",
    "\n",
    "### a. Predicting the Class of the Sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentence, model):\n",
    "    # Predict the class of the sentence\n",
    "    p = bow(sentence, words, show_details=False)\n",
    "    res = model.predict(np.array([p]))[0]\n",
    "    ERROR_THRESHOLD = 0.25\n",
    "    results = [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return_list = []\n",
    "    for r in results:\n",
    "        return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])})\n",
    "    return return_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- bow(sentence, words, show_details=False): Converts the sentence into a bag of words representation.\n",
    "- model.predict(np.array([p]))[0]: Predicts the intent using the pre-trained model.\n",
    "- ERROR_THRESHOLD = 0.25: Sets the minimum confidence threshold for predictions.\n",
    "- [[i, r] for i, r in enumerate(res) if r > ERROR_THRESHOLD]: Filters results to include only those with confidence above the threshold.\n",
    "- results.sort(key=lambda x: x[1], reverse=True): Sorts the results by probability in descending order.\n",
    "- return_list.append({\"intent\": classes[r[0]], \"probability\": str(r[1])}): Prepares the list of predicted intents with probabilities.\n",
    "\n",
    "### b. Getting the Response for the Predicted Intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response(ints, intents_json):\n",
    "    # Get the response for the predicted intent\n",
    "    tag = ints[0]['intent']\n",
    "    list_of_intents = intents_json['intents']\n",
    "    for i in list_of_intents:\n",
    "        if i['tag'] == tag:\n",
    "            result = random.choice(i['responses'])\n",
    "            break\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- tag = ints[0]['intent']: Retrieves the predicted intent from the list of intents.\n",
    "- list_of_intents = intents_json['intents']: Accesses the list of all intents in the intents JSON file.\n",
    "- random.choice(i['responses']): Chooses a random response from the list of responses for the predicted intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Generating Chatbot Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chatbot_response(msg):\n",
    "    ints = predict_class(msg, model)\n",
    "    if len(ints) == 0:  # Check if no intents are predicted\n",
    "        return \"Sorry, I didn't quite understand that. Could you try again?\"\n",
    "    res = get_response(ints, intents)  # Ensure correct function name is used\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- predict_class(msg, model): Predicts the intent of the user's message.\n",
    "- if len(ints) == 0:: Checks if no intents were predicted (i.e., the model couldn't recognize the message).\n",
    "- get_response(ints, intents): Retrieves the corresponding response based on the predicted intent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Flask Routes and Web Interface\n",
    "The following section sets up the routes and user interface using Flask to handle user interactions with the chatbot.\n",
    "\n",
    "### a. Setting Up the Flask Application\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flask application setup\n",
    "app = Flask(__name__)\n",
    "app.static_folder = 'static'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- Flask(__name__): Initializes the Flask application.\n",
    "- app.static_folder = 'static': Specifies the folder for static files like images, CSS, and JavaScript."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Home Route (Chatbot Interface)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/\")\n",
    "def home():\n",
    "    return render_template(\"index.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- @app.route(\"/\"): Defines the home route for the chatbot interface.\n",
    "- render_template(\"index.html\"): Renders the index.html template as the landing page for the user interface."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Chatbot Response Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/get\")\n",
    "def get_bot_response():\n",
    "    userText = request.args.get('msg')\n",
    "    return chatbot_response(userText)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- @app.route(\"/get\"): Defines the route for receiving messages from the user.\n",
    "- userText = request.args.get('msg'): Retrieves the user's input message sent as a query parameter.\n",
    "- chatbot_response(userText): Passes the message to the chatbot_response function and returns the response.\n",
    "\n",
    "### d. Login and Appointments Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/login\", methods=[\"GET\", \"POST\"])\n",
    "def login():\n",
    "    return render_template(\"login.html\")\n",
    "\n",
    "@app.route(\"/appointments\")\n",
    "def appointments():\n",
    "    # This is where you would show or create appointments\n",
    "    # Placeholder for now\n",
    "    return render_template(\"appointments.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- @app.route(\"/login\"): Defines the login route for users.\n",
    "- @app.route(\"/appointments\"): Defines the appointments route for logged-in users. The function currently renders a placeholder page.\n",
    "\n",
    "### e. Logout Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.route(\"/logout\")\n",
    "def logout():\n",
    "    return redirect(url_for('home'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- @app.route(\"/logout\"): Defines the logout route for users.\n",
    "- redirect(url_for('home')): Redirects the user back to the home page after logging out.\n",
    "\n",
    "### Running the Flask Application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(debug=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation:\n",
    "- if __name__ == \"__main__\":: Ensures the Flask application runs only when the script is executed directly (not when imported as a module).\n",
    "- app.run(debug=True): Starts the Flask development server in debug mode for easy debugging and live reloading."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "The Medical Chatbot Application is a Flask-based web application designed to assist users by predicting their intents through natural language processing. It utilizes a pre-trained Keras model to identify the intent of user input and generates relevant responses from a predefined set. Here's a brief summary of how the system works:\n",
    "\n",
    "1. User Interaction: Users send messages through the chatbot interface hosted on the Flask web application.\n",
    "2. Text Processing: The user's message is preprocessed by tokenizing, lemmatizing, and converting it into a bag of words representation.\n",
    "3. Intent Prediction: The processed input is passed to the trained model, which predicts the intent of the message based on predefined classes.\n",
    "4. Response Generation: The system matches the predicted intent with a response from the corresponding intent data in the JSON file.\n",
    "5. Web Interface: Flask routes manage the user login, chatbot interface, and appointments, ensuring seamless interaction between the user and the system.\n",
    "6. Web Application: The Flask application renders templates for user interactions, allowing users to chat with the bot, log in, and view appointments.\n",
    "\n",
    "This structure allows easy integration of NLP models with a web application, providing a powerful tool for user interaction in various domains like healthcare.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Notes\n",
    "\n",
    "#### Tokenization\n",
    "\n",
    "What is Tokenization?\n",
    "\n",
    "Tokenization is the process of breaking down a sentence into smaller units, called tokens. Tokens can be words, subwords, or even characters, but in most cases, tokenization refers to splitting text into individual words or phrases.\n",
    "\n",
    "Why is Tokenization Important in NLP?\n",
    "1. Structure: It helps structure raw text into manageable pieces.\n",
    "2. Analysis: Enables the model to understand the distinct elements (words or symbols) in a sentence.\n",
    "3. Consistency: Allows for the consistent processing of text by splitting it into parts that can be individually analyzed.\n",
    "\n",
    "Example:\n",
    "\n",
    "Let’s take the sentence:\n",
    "\n",
    "\"I am feeling great today!\"\n",
    "\n",
    "After tokenization, it would be split into the following tokens:\n",
    "[\"I\", \"am\", \"feeling\", \"great\", \"today\", \"!\"]\n",
    "\n",
    "In the Code:\n",
    "\n",
    "In your code, nltk.word_tokenize(sentence) is used to perform tokenization. It converts the sentence into a list of words that can be further processed.\n",
    "\n",
    "#### Lemmatization\n",
    "What is Lemmatization?\n",
    "\n",
    "Lemmatization is the process of reducing words to their base or root form, known as a lemma. Unlike stemming, which simply removes suffixes to form root words, lemmatization ensures the output is a valid word in the dictionary.\n",
    "\n",
    "Why is Lemmatization Important in NLP?\n",
    "- Standardization: Reduces inflected words to a common base form, improving the model's ability to recognize different forms of the same word.\n",
    "- Meaning Preservation: Lemmatization keeps the meaning of words intact. It ensures that variations of a word, such as \"running,\" \"ran,\" and \"runs,\" are all understood as the base form \"run.\"\n",
    "- Efficiency: Helps in reducing the number of unique words that need to be handled by the model.\n",
    "\n",
    "Example:\n",
    "\n",
    "For the sentence:\n",
    "\n",
    "\"The cats are running and the dogs ran.\"\n",
    "\n",
    "After lemmatization, it might convert to:\n",
    "[\"The\", \"cat\", \"are\", \"run\", \"and\", \"the\", \"dog\", \"run\"]\n",
    "\n",
    "In the Code:\n",
    "\n",
    "In your code, WordNetLemmatizer() is used to lemmatize words. It ensures that variations of words are reduced to their base form before being processed further.\n",
    "\n",
    "#### Key Intuitions in my Chatbot Code\n",
    "1. Tokenization: By breaking down the user's sentence into individual tokens, you can more effectively analyze the sentence. The model can look at each token (word) separately to understand its meaning and structure.\n",
    "2. Lemmatization: Once the sentence is tokenized, lemmatization ensures that variations of the same word (like \"running\" and \"run\") are treated as the same, improving the chatbot’s ability to identify intents accurately.\n",
    "\n",
    "These two techniques are fundamental to making the chatbot more intelligent by ensuring that it understands the core meaning of a sentence without being thrown off by different forms of words or sentence structures."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
